2025-08-16 00:35:19,604 - __main__ - INFO - Production test session started: test_20250816_003519
2025-08-16 00:35:19,604 - __main__ - INFO - Testing real API monitoring service
2025-08-16 00:35:20,032 - src.monitoring.api_clients - ERROR - FEMA API error: 400
2025-08-16 00:35:26,112 - ibm_watsonx_ai.client - INFO - Client successfully initialized
2025-08-16 00:35:26,902 - httpcore.connection - DEBUG - connect_tcp.started host='us-south.ml.cloud.ibm.com' port=443 local_address=None timeout=10 socket_options=None
2025-08-16 00:35:27,125 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x112997dd0>
2025-08-16 00:35:27,176 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x112926210> server_hostname='us-south.ml.cloud.ibm.com' timeout=10
2025-08-16 00:35:27,351 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x112997d50>
2025-08-16 00:35:27,351 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-08-16 00:35:27,351 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-08-16 00:35:27,352 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-08-16 00:35:27,352 - httpcore.http11 - DEBUG - send_request_body.complete
2025-08-16 00:35:27,352 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-08-16 00:35:28,335 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 16 Aug 2025 04:35:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server-Timing', b'intid;desc=5b35c01656c1f673'), (b'X-Global-Transaction-Id', b'c12fc2965d3d6fa5b8915d7027aed523'), (b'X-Xss-Protection', b'1; mode=block'), (b'X-Xss-Protection', b'1; mode=block'), (b'X-Content-Type-Options', b'nosniff'), (b'Referrer-Policy', b'strict-origin'), (b'Pragma', b'no-cache'), (b'X-Frame-Options', b'DENY'), (b'Content-Security-Policy', b"default-src 'none'; script-src 'self'; connect-src 'self'; img-src 'self'; style-src 'self'; frame-ancestors 'none'; form-action 'self';"), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains'), (b'Cache-Control', b'no-cache, no-store, must-revalidate'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'96fe39a02cca43b3-EWR'), (b'Content-Encoding', b'gzip')])
2025-08-16 00:35:28,335 - httpx - INFO - HTTP Request: GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-06&project_id=a51f2f6b-6910-4374-9d37-955868c6108d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 "HTTP/1.1 200 OK"
2025-08-16 00:35:28,335 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-08-16 00:35:28,338 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-08-16 00:35:28,338 - httpcore.http11 - DEBUG - response_closed.started
2025-08-16 00:35:28,338 - httpcore.http11 - DEBUG - response_closed.complete
2025-08-16 00:35:28,338 - ibm_watsonx_ai.wml_resource - INFO - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-06&project_id=a51f2f6b-6910-4374-9d37-955868c6108d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200'
2025-08-16 00:35:28,339 - ibm_watsonx_ai.wml_resource - DEBUG - Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-06&project_id=a51f2f6b-6910-4374-9d37-955868c6108d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200 200): {"total_count":23,"limit":200,"first":{"href":"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2025-08-06&project_id=a51f2f6b-6910-4374-9d37-955868c6108d&filters=function_text_generation%2C%21lifecycle_withdrawn%3Aand&limit=200"},"resources":[{"model_id":"google/flan-t5-xl","label":"flan-t5-xl-3b","provider":"Google","source":"Hugging Face","functions":[{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.","long_description":"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.","terms_url":"https://huggingface.co/google/flan-t5-xl/blob/main/README.md","input_tier":"class_1","output_tier":"class_1","number_params":"3b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization","tags":["function_prompt_tune_trainable"]},{"id":"retrieval_augmented_generation"},{"id":"classification","tags":["function_prompt_tune_trainable"]},{"id":"generation","tags":["function_prompt_tune_trainable"]},{"id":"extraction"}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095,"training_data_max_records":10000},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":4095},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":4095},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-12-07"},{"id":"deprecated","start_date":"2025-06-18"},{"id":"withdrawn","start_date":"2025-10-15"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"Input: {{input}} Output:"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":256},"max_output_tokens":{"default":128,"min":1,"max":128},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.3,"min":0.00001,"max":0.5}}},{"model_id":"ibm/granite-13b-instruct-v2","label":"granite-13b-instruct-v2","provider":"IBM","source":"IBM","functions":[{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883","input_tier":"class_1","output_tier":"class_1","number_params":"13b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","extraction","translation"],"tasks":[{"id":"question_answering","ratings":{"quality":3},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":0}],"tags":["Reasoning"]}]},{"id":"summarization","ratings":{"quality":2},"tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"Please write a summary highlighting the main points of the following text:"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":40,"min":1,"max":50},"verbalizer":{"default":"Please write a summary highlighting the main points of the following text: {{input}}"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":1,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":22.61}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":32.64}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.11}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":2},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.13}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":13.95}],"tags":["Reasoning"]}]},{"id":"classification","ratings":{"quality":3},"tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"Classify the text:"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"Input: {{input}} Output:"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":32,"min":1,"max":128},"learning_rate":{"default":0.0006,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":46.72}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":0}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":31.27}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":3.85}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":20.44}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":16.33}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53}],"tags":["Reasoning"]}]},{"id":"generation","tags":["function_prompt_tune_trainable"],"training_parameters":{"init_method":{"supported":["random","text"],"default":"text"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":13.21}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":2.42}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":43.71}],"tags":["Safety & Bias"]}]},{"id":"extraction","ratings":{"quality":2},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":25.21}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":0}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":12.34}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":8191,"training_data_max_records":10000},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8191},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8191},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8191}},"lifecycle":[{"id":"available","start_date":"2023-12-01"},{"id":"deprecated","start_date":"2025-06-18","alternative_model_ids":["ibm/granite-3-3-8b-instruct"]},{"id":"withdrawn","start_date":"2025-10-15","alternative_model_ids":["ibm/granite-3-3-8b-instruct"]}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":16,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.0002,"min":0.00001,"max":0.5}}},{"model_id":"ibm/granite-3-2-8b-instruct","label":"granite-3-2-8b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Granite 3.2 8b Instruct is a text-only model capable of reasoning which can be enabled or disabled by the user to use the right capability for the right use case.","long_description":"Granite 3.2 8b Instruct is a text-only model capable of reasoning which can be enabled or disabled by the user to use the right capability for the right use case.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"retrieval_augmented_generation"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"translation"},{"id":"function_calling","ratings":{"quality":3}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":16384},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":16384},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":16384},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2025-02-26"}],"versions":[{"version":"1.0.0","available_date":"2025-02-26"}]},{"model_id":"ibm/granite-3-2b-instruct","label":"granite-3-2b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883","input_tier":"class_c1","output_tier":"class_c1","number_params":"2b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling","code-generation","code-explanation","code-fixing"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":6}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":43.01}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":0},"metrics":[{"name":"F1 score","value":80.36}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":0},"metrics":[{"name":"F1 score","value":78.83}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":36.89}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":35.65}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":0},"metrics":[{"name":"F1 score","value":78.13}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":0},"metrics":[{"name":"F1 score","value":75.48}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":28.13}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":26.86}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.23}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":10.45}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":9.2}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":4.51}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":11.64}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.39}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":10}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":10.88}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":10.37}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":47.75}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":47.48}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"NDCG","value":83.37}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"NDCG","value":76.21}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"NDCG","value":80.93}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"NDCG","value":77.68}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":53.79}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":39.38}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.55}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":67.65}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":56}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":26.53}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":67.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.12}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.78}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":49.77}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.85}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":55.56}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":73.47}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":55.14}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.39}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.14}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":57.02}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":77.5}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":74.36}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":75.92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":64.17}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":72.03}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":73.11}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":37.75}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":44.99}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":43.64}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":38.55}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":39.5}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":45.41}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":46}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":37.15}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":45.71}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":44.39}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":37.99}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":39.65}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.53}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.06}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":57.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":60.94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":66.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":60.16}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":9.18}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":18.89}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":93.22}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":23.3}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":23.84}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":32.16}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":43.61}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":31.29}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":33.96}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":26.58}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":84.94}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":84.71}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":69.18}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":1},"metrics":[{"name":"String Containment","value":17.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":84.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":83.29}],"tags":["Translation"]}]},{"id":"function_calling"},{"id":"code-generation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.506}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP"},"metrics":[{"name":"pass@1","value":0.41}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.354}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.274}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.348}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.287}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.573}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.146}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP"},"metrics":[{"name":"pass@1","value":0.416}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Expands on the MBPP dataset with more Python programming problems and more comprehensive test cases. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP+"},"metrics":[{"name":"pass@1","value":0.521}],"tags":["Code generation"]}]},{"id":"code-explanation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.456}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.299}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.207}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.348}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.311}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.439}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.091}],"tags":["Code explanation"]}]},{"id":"code-fixing","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.518}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.146}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.177}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.183}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.226}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.341}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.055}],"tags":["Code fixing"]}]}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"},{"id":"deprecated","start_date":"2025-08-13","alternative_model_ids":["ibm/granite-3-3-8b-instruct"]},{"id":"withdrawn","start_date":"2025-11-12","alternative_model_ids":["ibm/granite-3-3-8b-instruct"]}],"versions":[{"version":"1.1.0","available_date":"2024-12-13"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-3-3-8b-instruct","label":"granite-3-3-8b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Granite-3.3-8b-Instruct is an IBM-trained, dense decoder-only models, which is particularly well-suited for generative tasks.","long_description":"Granite-3.3-8b-Instruct is designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. It employs a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"retrieval_augmented_generation"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"translation"},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":16384},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":16384},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":16384},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2025-04-16"}],"versions":[{"version":"3.3.0","available_date":"2025-04-16"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"ibm/granite-3-8b-instruct","label":"granite-3-8b-instruct","provider":"IBM","source":"IBM","functions":[{"id":"autoai_rag"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling","code-generation","code-explanation","code-fixing"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":54.23}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":0},"metrics":[{"name":"F1 score","value":86.97}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":85.67}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":49.39}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":46.73}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":82.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":83.1}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":31.41}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":28.96}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.04}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":12.42}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":11.58}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":4.15}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.44}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":17.29}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":11.04}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":16.3}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":12.39}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.82}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":51.26}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":81.77}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":76.79}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":0},"metrics":[{"name":"NDCG","value":82.2}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":78.24}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":57.84}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":53.27}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.55}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":74.39}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":47}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":52.18}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.86}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":82.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":79.69}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.84}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":90.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":63.25}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":69.83}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":75.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":83.68}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":71.05}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":63.06}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":67.51}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":82.99}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":81.97}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":86.07}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":73.64}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":81.48}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":82.5}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":41.72}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":52.39}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.74}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":42.62}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":44.3}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":53.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.95}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.51}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":51.18}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":43.45}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.3}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":53.22}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":53}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.09}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.66}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.44}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":77.34}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":11.18}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":11.06}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":93.33}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":24.66}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":28.95}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":43.88}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":48.5}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":40.31}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":44.23}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":32}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":91.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":88.82}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":75.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":29.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":90.47}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":90.24}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":3}},{"id":"code-generation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.646}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP"},"metrics":[{"name":"pass@1","value":0.496}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.518}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.354}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.543}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.421}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.713}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.311}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP"},"metrics":[{"name":"pass@1","value":0.472}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Expands on the MBPP dataset with more Python programming problems and more comprehensive test cases. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP+"},"metrics":[{"name":"pass@1","value":0.45}],"tags":["Code generation"]}]},{"id":"code-explanation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.572}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.494}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.341}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.628}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.488}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.64}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.305}],"tags":["Code explanation"]}]},{"id":"code-fixing","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.658}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.36}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.317}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.396}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.402}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.433}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.256}],"tags":["Code fixing"]}]}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"}],"versions":[{"version":"1.1.0","available_date":"2024-12-13"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-8b-code-instruct","label":"granite-8b-code-instruct","provider":"IBM","source":"IBM","functions":[{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883","input_tier":"class_1","output_tier":"class_1","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","code","extraction","code-generation","code-explanation","code-fixing"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"code-generation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.482}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.433}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.585}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.524}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.579}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.372}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.482}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.402}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.573}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.549}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.585}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.348}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP"},"metrics":[{"name":"pass@1","value":0.502}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Expands on the MBPP dataset with more Python programming problems and more comprehensive test cases. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP+"},"metrics":[{"name":"pass@1","value":0.571}],"tags":["Code generation"]}]},{"id":"code-explanation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.439}],"tags":["Code explanation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.366}],"tags":["Code explanation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.524}],"tags":["Code explanation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.427}],"tags":["Code explanation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.53}],"tags":["Code explanation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.165}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.396}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.396}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.476}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.39}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.506}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.22}],"tags":["Code explanation"]}]},{"id":"code-fixing","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.39}],"tags":["Code fixing"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.415}],"tags":["Code fixing"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.482}],"tags":["Code fixing"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.409}],"tags":["Code fixing"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.396}],"tags":["Code fixing"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.329}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.39}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.421}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.482}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.427}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.402}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.28}],"tags":["Code fixing"]}]}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":16384},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":16384},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":16384},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-05-09"}],"versions":[{"version":"1.1.1","available_date":"2024-10-28"},{"version":"1.1.0","available_date":"2024-09-03"},{"version":"1.0.0","available_date":"2024-05-09"}]},{"model_id":"ibm/granite-guardian-3-2b","label":"granite-guardian-3-2b","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en","input_tier":"class_c1","output_tier":"class_c1","number_params":"2b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"extraction"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"},{"id":"deprecated","start_date":"2025-07-09","alternative_model_ids":["ibm/granite-guardian-3-2-5b"]},{"id":"withdrawn","start_date":"2025-10-08","alternative_model_ids":["ibm/granite-guardian-3-2-5b"]}],"versions":[{"version":"1.1.0","available_date":"2025-01-15"},{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-guardian-3-8b","label":"granite-guardian-3-8b","provider":"IBM","source":"IBM","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.","long_description":"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en","input_tier":"class_12","output_tier":"class_12","number_params":"8b","min_shot_size":1,"task_ids":["question_answering","summarization","classification","generation","extraction"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"classification"},{"id":"generation"},{"id":"extraction"}],"model_limits":{"max_sequence_length":8192,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-21"},{"id":"deprecated","start_date":"2025-08-13"},{"id":"withdrawn","start_date":"2025-11-12"}],"versions":[{"version":"1.0.0","available_date":"2024-10-21"}]},{"model_id":"ibm/granite-vision-3-2-2b","label":"granite-vision-3-2-2b","provider":"IBM","source":"IBM","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Granite 3.2 Vision is a image-text-in, text-out model capable of understanding images like charts for enterprise use cases for computer vision tasks.","long_description":"Granite 3.2 Vision is a 2 billion image-text-in, text-out model capable of understanding images like charts, graphs, and infographics for enterprise use cases for computer vision tasks.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en","input_tier":"class_c1","output_tier":"class_c1","number_params":"2b","min_shot_size":1,"task_ids":["question_answering"],"tasks":[{"id":"question_answering"}],"model_limits":{"max_sequence_length":16384,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2025-02-26"},{"id":"deprecated","start_date":"2025-08-13"},{"id":"withdrawn","start_date":"2025-11-12"}],"versions":[{"version":"1.0.0","available_date":"2025-02-26"}]},{"model_id":"meta-llama/llama-2-13b-chat","label":"llama-2-13b-chat","provider":"Meta","source":"Hugging Face","functions":[{"id":"prompt_tune_inferable"},{"id":"prompt_tune_trainable"},{"id":"text_generation"}],"short_description":"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.","terms_url":"https://ai.meta.com/llama/license","input_tier":"class_1","output_tier":"class_1","number_params":"13b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3},"tags":["function_prompt_tune_trainable"]},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4},"tags":["function_prompt_tune_trainable"]},{"id":"generation","tags":["function_prompt_tune_trainable"]},{"id":"code"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":4096,"max_output_tokens":4095,"training_data_max_records":10000},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":4095},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":4095},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":4095}},"lifecycle":[{"id":"available","start_date":"2023-11-09"},{"id":"deprecated","start_date":"2024-08-26"}],"training_parameters":{"init_method":{"supported":["random","text"],"default":"random"},"init_text":{"default":"text"},"num_virtual_tokens":{"supported":[20,50,100],"default":100},"num_epochs":{"default":20,"min":1,"max":50},"verbalizer":{"default":"{{input}}"},"batch_size":{"default":8,"min":1,"max":16},"max_input_tokens":{"default":256,"min":1,"max":1024},"max_output_tokens":{"default":128,"min":1,"max":512},"torch_dtype":{"default":"bfloat16"},"accumulate_steps":{"default":16,"min":1,"max":128},"learning_rate":{"default":0.002,"min":0.00001,"max":0.5}}},{"model_id":"meta-llama/llama-3-2-11b-vision-instruct","label":"llama-3-2-11b-vision-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-11b-vision-instruc is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-11b-vision-instruc is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE","input_tier":"class_9","output_tier":"class_9","number_params":"11b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":14}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":53.89}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":77.85}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":75.69}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":43.36}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":50.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":78.78}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":73.36}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":31.72}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":23.79}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.21}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.5}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":16.35}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":4.65}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":20.03}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":18.68}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.23}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":17.8}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.8}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":49.32}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":41.4}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":83}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":77.84}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":82.27}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":79.66}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":56.74}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":46.07}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.73}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":75.44}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":49.68}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":37.76}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.81}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.84}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":84.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":83.59}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.03}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.06}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":72.2}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":62.98}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":67.84}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":83.13}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":79.15}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":67.23}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":64.25}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":78.23}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":79.84}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":83.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":82.79}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":81.97}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":77.87}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":75.21}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.38}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.24}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.39}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.75}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":48.34}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.8}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.94}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.9}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.38}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.9}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":43.37}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":48.66}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":51}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.99}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59.38}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.84}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":73.44}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":8.31}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":9.55}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":79.68}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":23.17}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":24.02}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":42.59}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":48.32}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":46.31}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":30.89}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":34.95}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":91.88}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":89.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":77.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":48}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":89.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":90.82}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-1b-instruct","label":"llama-3-2-1b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-1b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-1b-instruct is a pretrained and fine-tuned generative text model with 1 billion parameters, optimized for multilingual dialogue use cases and code output.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE","input_tier":"class_c1","output_tier":"class_c1","number_params":"1b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":1}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":24.11}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":60.7}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":59.13}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":28.91}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":26.42}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":60.68}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":60.44}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":25.94}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":21.74}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.32}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.96}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.31}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":4.47}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":18.68}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.35}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.9}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":16.85}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.34}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":39.37}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":34.33}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":83.62}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":77.13}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":79.42}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":67.74}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":40.07}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":25}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":48.27}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":47.99}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":25}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":33.83}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":23.47}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":32.03}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":44.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":25.27}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":32.37}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":34.48}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":52.68}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":36.89}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":28.25}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":21.23}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":34.08}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":47.58}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":52.21}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":52.17}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":45.22}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":47.11}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":42.92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":29.25}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.45}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.65}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":33.29}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.22}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":34.7}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.29}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":29.45}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.07}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":37.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":33.14}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.11}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.01}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.44}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":29.69}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":41.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":39.06}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":33.59}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":35.94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":23.44}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":32.03}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":52.34}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":5.23}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":1.13}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":76.35}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":16.65}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":15.51}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":31.63}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":39.14}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":29.65}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":24.44}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":24.44}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":64.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":63.88}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":51.06}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":10.59}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":66.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":67.06}],"tags":["Translation"]}]},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"},{"id":"deprecated","start_date":"2025-08-13","alternative_model_ids":["meta-llama/llama-4-maverick-17b-128e-instruct-fp8"]},{"id":"withdrawn","start_date":"2025-09-12","alternative_model_ids":["meta-llama/llama-4-maverick-17b-128e-instruct-fp8"]}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-3b-instruct","label":"llama-3-2-3b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-3b-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-3b-instruct is a pretrained and fine-tuned generative text model with 3 billion parameters, optimized for multilingual dialogue use cases and code output.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE","input_tier":"class_8","output_tier":"class_8","number_params":"3b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling","code-generation","code-explanation","code-fixing"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":9}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":48.49}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":80.76}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":82.55}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":41.84}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":44.21}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":75.98}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":76.34}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":30.26}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":23.05}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":7.13}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.24}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.78}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":4.74}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.97}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":17.47}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":10.92}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":16.16}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.72}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":50.52}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":39.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":84.52}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":75.63}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":82.88}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":74.86}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":49.71}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":34.18}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.18}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":76.01}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":56.95}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":33.67}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.5}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.12}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.66}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":80.47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":45.89}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.2}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":50.68}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":75.83}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":50.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":52.58}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":55.4}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.16}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":76.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":70.09}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":78.01}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":70.83}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":69.2}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":66.95}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":38.01}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":44.72}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.94}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":38.61}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":39.56}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.14}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.12}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":38.01}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.35}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":43.43}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":38.89}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.49}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.15}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":36.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":43.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55.47}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":47.66}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":40.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":50.78}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":60.94}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":7.09}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":4.31}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":78.08}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":22.96}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":21.71}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":36.15}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":40.46}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":40.51}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":38.46}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":31.9}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":79.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":79.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":67.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":21.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":81.06}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":81.41}],"tags":["Translation"]}]},{"id":"function_calling"},{"id":"code-generation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.451}],"tags":["Code generation"]},{"type":"academic","name":"Code","description":"Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP"},"metrics":[{"name":"pass@1","value":0.402}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.238}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.213}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.518}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.39}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.555}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalSynthesize"},"metrics":[{"name":"pass@1","value":0.256}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP"},"metrics":[{"name":"pass@1","value":0.396}],"tags":["Code generation"]},{"type":"watsonx.ai","name":"Code","description":"Expands on the MBPP dataset with more Python programming problems and more comprehensive test cases. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"MBPP+"},"metrics":[{"name":"pass@1","value":0.452}],"tags":["Code generation"]}]},{"id":"code-explanation","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.197}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.274}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.128}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.335}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.274}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.25}],"tags":["Code explanation"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalExplain"},"metrics":[{"name":"pass@1","value":0.159}],"tags":["Code explanation"]}]},{"id":"code-fixing","benchmarks":[{"type":"academic","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.552}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"C++","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.116}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Go","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.22}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Java","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.28}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"JavaScript","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.317}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Python","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.366}],"tags":["Code fixing"]},{"type":"watsonx.ai","name":"Code","description":"Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.","language":"Rust","dataset":{"name":"HumanEvalFix"},"metrics":[{"name":"pass@1","value":0.134}],"tags":["Code fixing"]}]}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"},{"id":"deprecated","start_date":"2025-08-13","alternative_model_ids":["meta-llama/llama-4-maverick-17b-128e-instruct-fp8"]},{"id":"withdrawn","start_date":"2025-09-12","alternative_model_ids":["meta-llama/llama-4-maverick-17b-128e-instruct-fp8"]}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-2-90b-vision-instruct","label":"llama-3-2-90b-vision-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-2-90b-vision-instruct is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-3-2-90b-vision-instruct is a pretrained and fine-tuned generative text model with 90 billion parameters, optimized for multilingual dialogue use cases and code output.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE","input_tier":"class_10","output_tier":"class_10","number_params":"90b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":33}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":60.68}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":82.01}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":85.83}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":30.99}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":56.06}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":86.49}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":77.48}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":32.87}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":20.81}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.96}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":17.22}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":19.08}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":6.27}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":22.05}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":19.65}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.38}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":21.29}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":17.18}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":56.77}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":43.99}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":84.88}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":77.56}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":79.78}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":76.15}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":56.44}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":75.13}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.73}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.92}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":67.26}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.06}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":96.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":80}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":77.69}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":82.5}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":90.98}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":86.53}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":84.21}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":77.12}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":86.07}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":88.26}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":91.13}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":87.8}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.71}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":88.16}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.4}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.27}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.89}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.8}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.1}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.17}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.51}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.43}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.78}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":57.64}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.62}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.17}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.97}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.28}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":75.78}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.56}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.56}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.84}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":10.27}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":26.01}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":75.84}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":26.25}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":27.21}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":45.94}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":53.54}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":54.11}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":41.38}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":41.01}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":92.94}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":91.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":81.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":52}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":93.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":92.12}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"meta-llama/llama-3-3-70b-instruct","label":"llama-3-3-70b-instruct","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"This version of Llama-3.3-70b-instruct is also the FP8 quantized version of the original FP16 weights.","long_description":"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE","input_tier":"class_13","output_tier":"class_13","number_params":"70b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4},"benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":25}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":60.62}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":78.39}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":78.49}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":44.79}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":55.57}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":81.03}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":73.72}],"tags":["Knowledge"]}]},{"id":"summarization","ratings":{"quality":3},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":33.04}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":29.95}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":9.3}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.48}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":16.4}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.52}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":20}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":24.72}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.99}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":18.24}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.34}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":51.17}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":41.94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":85.1}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":78.44}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":78.54}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":76.36}],"tags":["Information Retrieval"]}]},{"id":"classification","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":59.62}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":70.59}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.73}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":57.67}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.06}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":96.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":91.41}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":95.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":84.08}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":74.38}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":80}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":90.76}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":87.8}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":79.01}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":75.32}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":86.99}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":89.96}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":90.61}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":90.76}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":89.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":86.64}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":91.06}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.22}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.06}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.49}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.52}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.9}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.04}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.47}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.14}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.5}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.77}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":58.2}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.71}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.75}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":62.44}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":64.84}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":74.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.66}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":76.56}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.69}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.06}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":7.06}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":26.38}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":92.37}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":26.78}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":29.69}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":45.04}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":54.09}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","ratings":{"quality":4},"benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":52.75}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":46.55}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":40.56}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":93.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":91.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":83.06}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":54.12}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":93.88}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":92.35}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-12-06"}],"versions":[{"version":"3.3.0","available_date":"2024-12-06"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-3-405b-instruct","label":"llama-3-405b-instruct","provider":"Meta","source":"Meta","functions":[{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases","long_description":"Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases. It's also the largest open-sourced model ever released. It can also be used as a synthetic data generator, post-training data ranking judge, or model teacher/supervisor that can improve specialized capabilities in derivative, more inference friendly models.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE","input_tier":"class_3","output_tier":"class_7","number_params":"405b","min_shot_size":0,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":4096},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":4096},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":4096},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":4096}},"lifecycle":[{"id":"available","start_date":"2024-07-23"}],"versions":[{"version":"3.1.0","available_date":"2024-07-23"}]},{"model_id":"meta-llama/llama-4-maverick-17b-128e-instruct-fp8","label":"llama-4-maverick-17b-128e-instruct-fp8","provider":"Meta","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"image_chat"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama 4 Maverick, a 17 billion active parameter model with 128 experts.","long_description":"The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE","input_tier":"class_9","output_tier":"class_16","number_params":"400b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"retrieval_augmented_generation"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"translation"},{"id":"function_calling"}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2025-04-06"}],"versions":[{"version":"4.0.0","available_date":"2025-04-06"}],"supported_languages":["en","de","fr","it","pt","hi","es","th"]},{"model_id":"meta-llama/llama-guard-3-11b-vision","label":"llama-guard-3-11b-vision","provider":"Meta","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Llama-guard-3-11b-vision is an auto-regressive language model that uses an optimized transformer architecture.","long_description":"Llama-guard-3-11b-vision is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.","terms_url":"https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE","input_tier":"class_9","output_tier":"class_9","number_params":"11b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction"],"tasks":[{"id":"question_answering","ratings":{"quality":4}},{"id":"summarization","ratings":{"quality":3}},{"id":"retrieval_augmented_generation","ratings":{"quality":4}},{"id":"classification","ratings":{"quality":4}},{"id":"generation"},{"id":"code"},{"id":"extraction","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-09-25"}],"versions":[{"version":"3.2.0","available_date":"2024-09-25"}]},{"model_id":"mistralai/mistral-large","label":"mistral-large","provider":"Mistral AI","source":"Mistral","functions":[{"id":"autoai_rag"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Mistral Large, the most advanced Large Language Model (LLM) developed by Mistral Al, is an exceptionally powerful model. Thanks to its state-of-the-art reasoning capabilities it can be applied to any language-based task, including the most sophisticated ones.","long_description":"Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized. For comprehensive information and examples about this model, please refer to the release blog post.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883","input_tier":"mistral_large_input","output_tier":"mistral_large","number_params":"","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":35}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":64.06}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":87.97}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":89.08}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":53.04}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":51.47}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":84.21}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":84.16}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":34.31}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":26.75}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.47}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.99}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":17.79}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":3.28}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":15.88}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":16.7}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":10.86}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.95}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.21}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{"number_of_shots":2},"metrics":[{"name":"ROUGE-L","value":59.01}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":50.57}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":84.97}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":78.05}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":77.43}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":81.14}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{},"metrics":[{"name":"F1 score","value":61.66}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":66.67}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":96}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.44}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":61.28}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":70.41}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":97.66}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":94.53}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":93.75}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.83}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":77.11}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":84.9}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":91.94}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":88.98}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":83.33}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":82.79}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":89.34}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":89.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":91.5}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":93.6}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":92}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":87.8}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":89.43}],"tags":["Classification"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":67.62}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.02}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.69}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":60.83}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.06}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.12}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.66}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.5}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.23}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":68.47}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.67}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.2}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.04}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":69.47}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":71.88}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":79.69}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":82.03}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":78.91}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.94}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_english"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":85.16}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.97}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":8.92}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":44.08}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":65.34}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":27.29}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":30.38}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":50.19}],"tags":["Generation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":58.06}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":51.8}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":53.7}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":40.25}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":93.65}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":92.35}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":83.53}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":53.29}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":2},"metrics":[{"name":"String Containment","value":92.59}],"tags":["Translation"]},{"type":"academic","name":"Multilingual Language Understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":92.35}],"tags":["Translation"]}]},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":16384},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":16384},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":16384},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2024-07-09"},{"id":"deprecated","start_date":"2025-07-09","alternative_model_ids":["mistralai/mistral-medium-2505"]},{"id":"withdrawn","start_date":"2025-10-08","alternative_model_ids":["mistralai/mistral-medium-2505"]}],"versions":[{"version":"2.0.0","available_date":"2024-07-24"},{"version":"1.0.0","available_date":"2024-07-09"}],"supported_languages":["en","fr","de","it","zh","ja","ko","pt","nl","pl"]},{"model_id":"mistralai/mistral-medium-2505","label":"mistral-medium-2505","provider":"Mistral AI","source":"Mistral","functions":[{"id":"autoai_rag"},{"id":"image_chat"},{"id":"multilingual"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"Mistral Medium 3 (25.05) is our latest iteration of the Mistral Medium model family.","long_description":"Mistral Medium 3 (25.05) features multimodal capabilities and an extended context length of up to 128k. It can now process and understand visual inputs as well as long documents, further expanding its range of applications.","terms_url":"https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en","input_tier":"mistral_large_input","output_tier":"mistral_large","number_params":"","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation","function_calling"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"retrieval_augmented_generation"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"translation"},{"id":"function_calling","ratings":{"quality":4}}],"model_limits":{"max_sequence_length":131072,"max_output_tokens":16384},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":16384},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":16384},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2025-05-07"}],"versions":[{"version":"1.0.0","available_date":"2025-05-07"}],"supported_languages":["en","fr","de","it","zh","ja","ko","pt","nl","pl"]},{"model_id":"mistralai/mistral-small-3-1-24b-instruct-2503","label":"mistral-small-3-1-24b-instruct-2503","provider":"Mistral AI","source":"Hugging Face","functions":[{"id":"autoai_rag"},{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"This model is an instruction-finetuned version of: Mistral-Small-3.1-24B-Base-2503.","long_description":"Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.","terms_url":"https://www.apache.org/licenses/LICENSE-2.0","input_tier":"class_c1","output_tier":"class_17","number_params":"24b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","retrieval_augmented_generation","classification","generation","code","extraction","function_calling"],"tasks":[{"id":"question_answering"},{"id":"summarization"},{"id":"retrieval_augmented_generation"},{"id":"classification"},{"id":"generation"},{"id":"code"},{"id":"extraction"},{"id":"function_calling"}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":16384},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":16384},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":16384},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":16384}},"lifecycle":[{"id":"available","start_date":"2025-04-22"}],"versions":[{"version":"1.0.0","available_date":"2025-04-22"}]},{"model_id":"mistralai/pixtral-12b","label":"pixtral-12b","provider":"Mistral AI","source":"Hugging Face","functions":[{"id":"image_chat"},{"id":"text_chat"},{"id":"text_generation"}],"short_description":"The Pixtral-12b is a Multimodal Model of 12B parameters plus a 400M parameter vision encoder.","long_description":"Pixtral-12b is a 12-billion parameter model pre-trained and fine-tuned for generative tasks in text and image domains. It is optimized for multilingual use cases and provides robust performance in creative content generation.","terms_url":"https://www.apache.org/licenses/LICENSE-2.0","input_tier":"class_9","output_tier":"class_9","number_params":"12b","min_shot_size":1,"task_ids":["question_answering","summarization","retrieval_augmented_generation","classification","generation","code","extraction","translation"],"tasks":[{"id":"question_answering","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"FinQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"Accuracy","value":31}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":54.09}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"French","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":88.7}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"German","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":89.65}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":54.12}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":50.94}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":84.42}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MKQA"},"prompt":{"number_of_shots":2},"metrics":[{"name":"F1 score","value":86.52}],"tags":["Knowledge"]}]},{"id":"summarization","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.","language":"English","dataset":{"name":"Summarization"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":27.9}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that summarizes US Congressional and California state bills from 1993–2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"BillSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":19.89}],"tags":["Summarization"]},{"type":"academic","name":"bluebench","description":"Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.","language":"English","dataset":{"name":"TLDR17"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":8.19}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.68}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"German","dataset":{"name":"MLSUM"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.61}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Arabic","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":2.61}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"French","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":18.11}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Japanese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.81}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Korean","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":10.02}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Portuguese","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":14.63}],"tags":["Summarization"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.","language":"Spanish","dataset":{"name":"XLSum"},"prompt":{"number_of_shots":0},"metrics":[{"name":"ROUGE-L","value":13.8}],"tags":["Summarization"]}]},{"id":"retrieval_augmented_generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.","language":"English","dataset":{"name":"RAG"},"prompt":{},"metrics":[{"name":"ROUGE-L","value":43.04}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.","language":"English","dataset":{"name":"CLAP NQ"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":47.34}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"French","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":84.95}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"German","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":78.12}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Portuguese","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":79.99}],"tags":["Information Retrieval"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.","language":"Spanish","dataset":{"name":"XGLUE.wpr"},"prompt":{"number_of_shots":5},"metrics":[{"name":"NDCG","value":74.53}],"tags":["Information Retrieval"]}]},{"id":"classification","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.","language":"English","dataset":{"name":"Classification"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":56.76}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"20 Newsgroups"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":49.25}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"BBQ"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.18}],"tags":["Safety & Bias"]},{"type":"academic","name":"bluebench","description":"Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.","language":"English","dataset":{"name":"CFPB"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":76.19}],"tags":["Classification"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"HellaSwag"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":59}],"tags":["Reasoning"]},{"type":"academic","name":"bluebench","description":"Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.","language":"English","dataset":{"name":"LegalBench"},"prompt":{"number_of_shots":1},"metrics":[{"name":"F1 score","value":52}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"MMLU-Pro"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.92}],"tags":["Knowledge"]},{"type":"academic","name":"bluebench","description":"Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.","language":"English","dataset":{"name":"OpenBookQA"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":89.06}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":90.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":92.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":86.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":86.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":89.84}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"Belebele"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":90.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":73.03}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":73.86}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":68.88}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":89.52}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":82.26}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":73.03}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":72.73}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Arabic","dataset":{"name":"MASSIVE_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":75.63}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"French","dataset":{"name":"MASSIVE_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.71}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"German","dataset":{"name":"MASSIVE_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":86.31}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Japanese","dataset":{"name":"MASSIVE_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":91.13}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Korean","dataset":{"name":"MASSIVE_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":86.78}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Portuguese","dataset":{"name":"MASSIVE_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":85.48}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.","language":"Spanish","dataset":{"name":"MASSIVE_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":87.24}],"tags":["Classification"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":48.88}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.75}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.49}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":45.48}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":51.75}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54.86}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55.04}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XMMLU_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.67}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XMMLU_en"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":55.3}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XMMLU_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":54.34}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Japanese","dataset":{"name":"XMMLU_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":46.45}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Korean","dataset":{"name":"XMMLU_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":52.79}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XMMLU_en"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Accuracy","value":55.05}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XMMLU_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":55.55}],"tags":["Knowledge"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":42.19}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":53.12}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":49.22}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Arabic","dataset":{"name":"XNLI_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":63.28}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"French","dataset":{"name":"XNLI_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":65.62}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"German","dataset":{"name":"XNLI_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":61.72}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.","language":"Spanish","dataset":{"name":"XNLI_en"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":67.97}],"tags":["Reasoning"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.","language":"Portuguese","dataset":{"name":"XWinograd"},"prompt":{"number_of_shots":5},"metrics":[{"name":"Accuracy","value":72.66}],"tags":["Reasoning"]}]},{"id":"generation","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.","language":"English","dataset":{"name":"Generation"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":11.63}],"tags":["Generation"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.","language":"English","dataset":{"name":"Arena-Hard"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Win rate","value":8.58}],"tags":["Chatbot Ability"]},{"type":"academic","name":"bluebench","description":"Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.","language":"English","dataset":{"name":"AttaQ 500"},"prompt":{"number_of_shots":0},"metrics":[{"name":"Safety","value":52.26}],"tags":["Safety & Bias"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"French","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":26.53}],"tags":["Generation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"German","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":27.86}],"tags":["Generation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Portuguese","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":46.51}],"tags":["Generation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.","language":"Spanish","dataset":{"name":"XGLUE.qg"},"prompt":{"number_of_shots":5},"metrics":[{"name":"ROUGE-L","value":55.1}],"tags":["Generation"]}]},{"id":"code"},{"id":"extraction","benchmarks":[{"type":"watsonx.ai","name":"Language Understanding","description":"Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.","language":"English","dataset":{"name":"Extraction"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":42.41}],"tags":["Extraction"]},{"type":"academic","name":"bluebench","description":"Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.","language":"English","dataset":{"name":"Universal NER"},"prompt":{"number_of_shots":5},"metrics":[{"name":"F1 score","value":49.06}],"tags":["Extraction"]}]},{"id":"translation","benchmarks":[{"type":"academic","name":"bluebench","description":"Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.","language":"English","dataset":{"name":"FLoRes-101"},"prompt":{"number_of_shots":5},"metrics":[{"name":"SacreBLEU","value":38.33}],"tags":["Translation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"French","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":0},"metrics":[{"name":"String Containment","value":90.12}],"tags":["Translation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"German","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":88.12}],"tags":["Translation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Japanese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":79.18}],"tags":["Translation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Korean","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":44.24}],"tags":["Translation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Portuguese","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":5},"metrics":[{"name":"String Containment","value":89.41}],"tags":["Translation"]},{"type":"academic","name":"Multilingual language understanding","description":"Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.","language":"Spanish","dataset":{"name":"Basic English"},"prompt":{"number_of_shots":10},"metrics":[{"name":"String Containment","value":90.59}],"tags":["Translation"]}]}],"model_limits":{"max_sequence_length":128000,"max_output_tokens":8192},"limits":{"3f6acf43-ede8-413a-ac69-f8af3bb0cbfe":{"call_time":"5m0s","max_output_tokens":8192},"a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577":{"call_time":"10m0s","max_output_tokens":8192},"d18d88b9-be7a-46ec-be1e-aff14904f1e9":{"call_time":"10m0s","max_output_tokens":8192}},"lifecycle":[{"id":"available","start_date":"2024-10-01"},{"id":"deprecated","start_date":"2025-07-09","alternative_model_ids":["mistralai/mistral-small-3-1-24b-instruct-2503"]},{"id":"withdrawn","start_date":"2025-10-08","alternative_model_ids":["mistralai/mistral-small-3-1-24b-instruct-2503"]}],"versions":[{"version":"1.0.0","available_date":"2024-10-01"}]}]}
2025-08-16 00:35:28,453 - src.monitoring.planning_agents - ERROR - WatsonX deployment planning error: 'dict' object has no attribute 'availability_status'
2025-08-16 00:35:28,453 - __main__ - ERROR - WatsonX integration test failed: 'dict' object has no attribute 'response_time_minutes'
2025-08-16 00:35:28,464 - src.workflows.detection_workflow - INFO - Workflow phase: monitoring, next action: poll_apis
2025-08-16 00:35:28,465 - src.workflows.detection_workflow - ERROR - Workflow execution error: No synchronous function provided to "api_monitoring".
Either initialize with a synchronous function or invoke via the async API (ainvoke, astream, etc.)
2025-08-16 00:35:28,476 - src.workflows.planning_workflow - INFO - Planning phase: monitoring, next action: poll_apis
2025-08-16 00:35:28,476 - src.workflows.planning_workflow - ERROR - Planning workflow execution error: No synchronous function provided to "load_planning_data".
Either initialize with a synchronous function or invoke via the async API (ainvoke, astream, etc.)
2025-08-16 00:35:28,480 - src.workflows.planning_workflow - INFO - Starting integrated disaster management cycle
2025-08-16 00:35:28,480 - src.workflows.planning_workflow - INFO - Phase 1: Running disaster detection workflow
2025-08-16 00:35:28,490 - src.workflows.detection_workflow - INFO - Workflow phase: monitoring, next action: poll_apis
2025-08-16 00:35:28,490 - src.workflows.detection_workflow - ERROR - Workflow execution error: No synchronous function provided to "api_monitoring".
Either initialize with a synchronous function or invoke via the async API (ainvoke, astream, etc.)
2025-08-16 00:35:28,490 - src.workflows.planning_workflow - INFO - No planning workflow trigger - detection cycle complete
2025-08-16 00:35:28,518 - httpcore.connection - DEBUG - close.started
2025-08-16 00:35:28,518 - httpcore.connection - DEBUG - close.complete
